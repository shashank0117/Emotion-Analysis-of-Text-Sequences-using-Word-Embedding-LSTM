{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w...\n",
       "5       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6     sadness  I should be sleep, but im not! thinking about ...\n",
       "7       worry               Hmmm. http://www.djhero.com/ is down\n",
       "8     sadness            @charviray Charlene my love. I miss you\n",
       "9     sadness         @kelcouch I'm sorry  at least it's Friday?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"text_emotion.csv\")\n",
    "df = df.drop([\"tweet_id\", \"author\"], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = []\n",
    "universal_word_list = []\n",
    "maximum_length = 0\n",
    "for i in df.content:\n",
    "    local = \"\"\n",
    "    flag = 0\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  i).split()\n",
    "    for j in wordList:\n",
    "        if not wordnet.synsets(j):\n",
    "            continue\n",
    "        elif \"@\" in j:\n",
    "            flag = flag + 1\n",
    "            local = local + \"[USER] \"\n",
    "        else:\n",
    "            flag = flag + 1\n",
    "            j = j.lower()\n",
    "            if j not in universal_word_list:\n",
    "                universal_word_list.append(j)\n",
    "            local = local + j + \" \"\n",
    "    if maximum_length <= flag:\n",
    "        maximum_length = flag\n",
    "    sentence.append(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i know i was bad habit earlier i started at part ', 'n bed a headache on call ', 'funeral ceremony gloomy friday ', 'wants hang out friends soon ', 'want trade someone who has houston tickets but no one will ', 're pinging why t go prom bc t like friends ', 'i be sleep but not thinking about an old friend who i want but he s married now damn amp he wants me 2 scandalous ', 'http www is down ', 'love i miss ', 'i m sorry at least it s friday ']\n",
      "Maximum length of the sentence in the dataset 31\n",
      "                                           sentences\n",
      "0  i know i was bad habit earlier i started at part \n",
      "1                          n bed a headache on call \n",
      "2                    funeral ceremony gloomy friday \n",
      "3                       wants hang out friends soon \n",
      "4  want trade someone who has houston tickets but...\n",
      "5        re pinging why t go prom bc t like friends \n",
      "6  i be sleep but not thinking about an old frien...\n",
      "7                                  http www is down \n",
      "8                                       love i miss \n",
      "9                    i m sorry at least it s friday \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>i know i was bad habit earlier i started at part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>n bed a headache on call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants hang out friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>want trade someone who has houston tickets but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>re pinging why t go prom bc t like friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i be sleep but not thinking about an old frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>http www is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>love i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i m sorry at least it s friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  i know i was bad habit earlier i started at part \n",
       "1     sadness                          n bed a headache on call \n",
       "2     sadness                    funeral ceremony gloomy friday \n",
       "3  enthusiasm                       wants hang out friends soon \n",
       "4     neutral  want trade someone who has houston tickets but...\n",
       "5       worry        re pinging why t go prom bc t like friends \n",
       "6     sadness  i be sleep but not thinking about an old frien...\n",
       "7       worry                                  http www is down \n",
       "8     sadness                                       love i miss \n",
       "9     sadness                    i m sorry at least it s friday "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence[:10])\n",
    "print(\"Maximum length of the sentence in the dataset\", maximum_length)\n",
    "content = pd.DataFrame({'sentences':sentence})\n",
    "print(content.head(10))\n",
    "df = df.drop([\"content\"], axis=1)\n",
    "df['content'] = content.values\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>n bed a headache on call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants hang out friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>want trade someone who has houston tickets but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>re pinging why t go prom bc t like friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i be sleep but not thinking about an old frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>http www is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>love i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i m sorry at least it s friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                            content\n",
       "1      sadness                          n bed a headache on call \n",
       "2      sadness                    funeral ceremony gloomy friday \n",
       "3   enthusiasm                       wants hang out friends soon \n",
       "4      neutral  want trade someone who has houston tickets but...\n",
       "5        worry        re pinging why t go prom bc t like friends \n",
       "6      sadness  i be sleep but not thinking about an old frien...\n",
       "7        worry                                  http www is down \n",
       "8      sadness                                       love i miss \n",
       "9      sadness                    i m sorry at least it s friday \n",
       "10     neutral                                  cant fall asleep "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how='all')\n",
    "for i in range(0, len(df)):\n",
    "    if df.sentiment[i] == \"empty\":\n",
    "        df = df.drop(i)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'know', 'was', 'bad', 'habit', 'earlier', 'started', 'at', 'part', 'n']\n",
      "['sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun' 'hate'\n",
      " 'happiness' 'boredom' 'relief' 'anger']\n"
     ]
    }
   ],
   "source": [
    "sentiments = df.sentiment.unique()\n",
    "print(universal_word_list[:10])\n",
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_word = {i:universal_word_list[i] for i in range(0, len(universal_word_list))}\n",
    "word_to_key = {val:key for (key, val) in key_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= 40)\n",
    "tokenizer.fit_on_texts(df['content'])\n",
    "sequences = tokenizer.texts_to_sequences(df['content'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "flag = pd.Series(list(df['sentiment']))\n",
    "Y = pd.get_dummies(flag)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(X)\n",
    "ratio = 0.9\n",
    "flag = int(total*ratio)\n",
    "X_train = X[:flag]\n",
    "X_test = X[flag:]\n",
    "Y_train = Y[:flag]\n",
    "Y_test = Y[flag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from numpy import array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><B>Bag of Words model</B></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                252       \n",
      "=================================================================\n",
      "Total params: 44,702\n",
      "Trainable params: 44,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100,input_shape=(50,1),return_sequences=False))\n",
    "model.add(Dense(30,kernel_initializer=\"normal\",activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,kernel_initializer=\"normal\",activation=\"relu\"))\n",
    "model.add(Dense(len(sentiments),kernel_initializer=\"normal\",activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer =\"adam\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37214 samples, validate on 1959 samples\n",
      "Epoch 1/1\n",
      "37214/37214 [==============================] - 27s 728us/step - loss: 2.1331 - acc: 0.2205 - val_loss: 2.1702 - val_acc: 0.1149\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape((len(X), 50, 1))\n",
    "model.fit(X,Y,epochs=1,batch_size=128,validation_split=0.05);\n",
    "model.save('train_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('playground_model.h5')\n",
    "yhat = model.predict(X, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1><B>Word Embeddings model</B></H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 500)           7813000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 31)                65968     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               8192      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                3084      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 7,890,244\n",
      "Trainable params: 7,890,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35255 samples, validate on 3918 samples\n",
      "Epoch 1/10\n",
      "35255/35255 [==============================] - 45s 1ms/step - loss: 2.1042 - acc: 0.2286 - val_loss: 2.1130 - val_acc: 0.2420\n",
      "Epoch 2/10\n",
      "35255/35255 [==============================] - 43s 1ms/step - loss: 2.0317 - acc: 0.2822 - val_loss: 2.0566 - val_acc: 0.2790\n",
      "Epoch 3/10\n",
      "35255/35255 [==============================] - 42s 1ms/step - loss: 2.0138 - acc: 0.2941 - val_loss: 2.0428 - val_acc: 0.2958\n",
      "Epoch 4/10\n",
      "35255/35255 [==============================] - 43s 1ms/step - loss: 2.0066 - acc: 0.2963 - val_loss: 2.0374 - val_acc: 0.2935\n",
      "Epoch 5/10\n",
      "35255/35255 [==============================] - 43s 1ms/step - loss: 2.0039 - acc: 0.2970 - val_loss: 2.0344 - val_acc: 0.2938\n",
      "Epoch 6/10\n",
      "35255/35255 [==============================] - 43s 1ms/step - loss: 2.0000 - acc: 0.2979 - val_loss: 2.0752 - val_acc: 0.2741\n",
      "Epoch 7/10\n",
      "35255/35255 [==============================] - 42s 1ms/step - loss: 1.9986 - acc: 0.2982 - val_loss: 2.0342 - val_acc: 0.2956\n",
      "Epoch 8/10\n",
      "35255/35255 [==============================] - 42s 1ms/step - loss: 1.9960 - acc: 0.2999 - val_loss: 2.0590 - val_acc: 0.2892\n",
      "Epoch 9/10\n",
      "35255/35255 [==============================] - 42s 1ms/step - loss: 1.9922 - acc: 0.2989 - val_loss: 2.0382 - val_acc: 0.2930\n",
      "Epoch 10/10\n",
      "35255/35255 [==============================] - 42s 1ms/step - loss: 1.9886 - acc: 0.2993 - val_loss: 2.0538 - val_acc: 0.2871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2406eeb1240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(len(X), 50)\n",
    "inputs = Input(shape=[50])\n",
    "layer = Embedding(len(universal_word_list),500,input_length=50)(inputs)\n",
    "layer = LSTM(maximum_length)(layer)\n",
    "layer = Dense(256)(layer)\n",
    "layer = Activation('relu')(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(len(sentiments))(layer)\n",
    "layer = Activation('sigmoid')(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(X,Y,batch_size=128,epochs=10,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15718247262839580044\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6612275691\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1189425149607929321\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:08:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Reducing the classes to improve the accuracy of the model to be trained</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "two = df\n",
    "categories = pd.get_dummies(df.sentiment)\n",
    "two = two.drop(\"sentiment\", axis = 1)\n",
    "two = pd.concat([two, categories], axis=1, sort=False)\n",
    "two = two.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_happy = []\n",
    "is_sad = []\n",
    "is_neutral = []\n",
    "\n",
    "def update(happy, sad, neutral):\n",
    "    is_happy.append(happy)\n",
    "    is_sad.append(sad)\n",
    "    is_neutral.append(neutral)\n",
    "\n",
    "for i in two.values:\n",
    "    if i[5] == 1 or i[3] == 1 or i[4] == 1 or i[7] == 1 or i[11] == 1:\n",
    "        update(1, 0, 0)\n",
    "    elif i[10] == 1 or i[1] == 1 or i[12] == 1 or i[6] == 1 or i[2] == 1:\n",
    "        update(0, 1, 0)\n",
    "    elif i[8] == 1 or i[9] == 1:\n",
    "        update(0, 0, 1)\n",
    "    else:\n",
    "        update(0, 0, 0)\n",
    "\n",
    "is_happy = pd.DataFrame(is_happy)\n",
    "is_sad = pd.DataFrame(is_sad)\n",
    "is_neutral = pd.DataFrame(is_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n bed a headache on call</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wants hang out friends soon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want trade someone who has houston tickets but...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re pinging why t go prom bc t like friends</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i be sleep but not thinking about an old frien...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http www is down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>love i miss</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i m sorry at least it s friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cant fall asleep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>choked on retainers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i have beat stupid song get next rude</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>u watch hills in london u will realise it is w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>got news</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>storm is here electricity is gone</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>agreed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>so sleepy again it s not even late i fail once...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lady gaga tweeted about not being impressed by...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>are convinced i have always wanted signals did...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>oh too bad i hope it gets better i been having...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wondering why i m awake at writing a new song ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>no topic maps talks at markup conference progr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i ate i don t know it is why do i keep telling...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>so tired i think i m definitely going get an e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>on way home n having 2 deal w underage girls d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i m sorry people are so rude isaac get some ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>servers still down i need hit 80 before all pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fudge just bs d whole paper so tired i hate sc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i hate cancer i hate it i hate it i hate it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>it is so annoying starts typing on computer in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39143</th>\n",
       "      <td>knows a site like swedish site sl se or great ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39144</th>\n",
       "      <td>happy birthday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39145</th>\n",
       "      <td>sarah u hope u remember me</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39146</th>\n",
       "      <td>good luck backpack quite helping</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39147</th>\n",
       "      <td>good morning midday nation formula one in one ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39148</th>\n",
       "      <td>pretty lady happy mother s day s mother future...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39149</th>\n",
       "      <td>right coursework now promise</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39150</th>\n",
       "      <td>say</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39151</th>\n",
       "      <td>bloody feds lost last statement r hounding me ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39152</th>\n",
       "      <td>awesome get</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39153</th>\n",
       "      <td>sitting in going home a week cant wait see fam...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39154</th>\n",
       "      <td>good luck auction</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39155</th>\n",
       "      <td>guys have ask just ask okay accept critics com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39156</th>\n",
       "      <td>not really just leaving flat now on lookout lu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39157</th>\n",
       "      <td>oh s why</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39158</th>\n",
       "      <td>husband is golfing amp toddler i frolic am i c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39159</th>\n",
       "      <td>going watch boy in striped s hope i don t cry</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39160</th>\n",
       "      <td>gave bikes a thorough wash degrease it grease ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39161</th>\n",
       "      <td>had such amazing time last night were incredible</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39162</th>\n",
       "      <td>snoring is so annoying n it keeps me sleeping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39163</th>\n",
       "      <td>i think lesson day is not have luggage</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39164</th>\n",
       "      <td>can give me link diaries please</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39165</th>\n",
       "      <td>showing french skills things good over here lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39166</th>\n",
       "      <td>yeah twitter has many uses me it s just know i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39167</th>\n",
       "      <td>following</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39168</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39169</th>\n",
       "      <td>happy mothers day all love</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39170</th>\n",
       "      <td>happy mother s day all mommies out there be wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39171</th>\n",
       "      <td>beautiful follow me peep out new hit singles w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39172</th>\n",
       "      <td>bullet train tokyo i have been visiting japan ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39173 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  0  0  0\n",
       "0                              n bed a headache on call   1  0  0\n",
       "1                        funeral ceremony gloomy friday   1  0  0\n",
       "2                           wants hang out friends soon   1  0  0\n",
       "3      want trade someone who has houston tickets but...  0  0  1\n",
       "4            re pinging why t go prom bc t like friends   0  0  0\n",
       "5      i be sleep but not thinking about an old frien...  1  0  0\n",
       "6                                      http www is down   0  0  0\n",
       "7                                           love i miss   1  0  0\n",
       "8                        i m sorry at least it s friday   1  0  0\n",
       "9                                      cant fall asleep   0  0  1\n",
       "10                                  choked on retainers   0  0  0\n",
       "11                i have beat stupid song get next rude   1  0  0\n",
       "12     u watch hills in london u will realise it is w...  1  0  0\n",
       "13                                             got news   0  1  0\n",
       "14                    storm is here electricity is gone   1  0  0\n",
       "15                                               agreed   0  0  1\n",
       "16     so sleepy again it s not even late i fail once...  1  0  0\n",
       "17     lady gaga tweeted about not being impressed by...  0  0  0\n",
       "18     are convinced i have always wanted signals did...  1  0  0\n",
       "19     oh too bad i hope it gets better i been having...  0  0  0\n",
       "20     wondering why i m awake at writing a new song ...  1  0  0\n",
       "21     no topic maps talks at markup conference progr...  0  0  1\n",
       "22     i ate i don t know it is why do i keep telling...  0  0  0\n",
       "23     so tired i think i m definitely going get an e...  1  0  0\n",
       "24     on way home n having 2 deal w underage girls d...  0  0  0\n",
       "25     i m sorry people are so rude isaac get some ma...  1  0  0\n",
       "26     servers still down i need hit 80 before all pa...  0  0  0\n",
       "27     fudge just bs d whole paper so tired i hate sc...  1  0  0\n",
       "28          i hate cancer i hate it i hate it i hate it   0  0  0\n",
       "29     it is so annoying starts typing on computer in...  1  0  0\n",
       "...                                                  ... .. .. ..\n",
       "39143  knows a site like swedish site sl se or great ...  0  0  1\n",
       "39144                                    happy birthday   0  0  1\n",
       "39145                        sarah u hope u remember me   1  0  0\n",
       "39146                  good luck backpack quite helping   0  1  0\n",
       "39147  good morning midday nation formula one in one ...  1  0  0\n",
       "39148  pretty lady happy mother s day s mother future...  0  0  1\n",
       "39149                      right coursework now promise   0  0  1\n",
       "39150                                               say   0  1  0\n",
       "39151  bloody feds lost last statement r hounding me ...  0  0  0\n",
       "39152                                       awesome get   0  1  0\n",
       "39153  sitting in going home a week cant wait see fam...  0  1  0\n",
       "39154                                 good luck auction   0  1  0\n",
       "39155  guys have ask just ask okay accept critics com...  0  0  1\n",
       "39156  not really just leaving flat now on lookout lu...  0  0  1\n",
       "39157                                          oh s why   0  1  0\n",
       "39158  husband is golfing amp toddler i frolic am i c...  0  1  0\n",
       "39159     going watch boy in striped s hope i don t cry   0  1  0\n",
       "39160  gave bikes a thorough wash degrease it grease ...  0  1  0\n",
       "39161  had such amazing time last night were incredible   0  1  0\n",
       "39162  snoring is so annoying n it keeps me sleeping ...  0  0  1\n",
       "39163            i think lesson day is not have luggage   0  0  1\n",
       "39164                   can give me link diaries please   0  0  1\n",
       "39165  showing french skills things good over here lo...  0  0  1\n",
       "39166  yeah twitter has many uses me it s just know i...  0  0  1\n",
       "39167                                         following   0  1  0\n",
       "39168                                                     0  0  1\n",
       "39169                        happy mothers day all love   0  0  1\n",
       "39170  happy mother s day all mommies out there be wo...  0  0  1\n",
       "39171  beautiful follow me peep out new hit singles w...  0  1  0\n",
       "39172  bullet train tokyo i have been visiting japan ...  0  0  1\n",
       "\n",
       "[39173 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two = two.drop([\"index\", \"anger\", \"boredom\", \"enthusiasm\", \"fun\", \"happiness\", \"hate\", \"love\", \"neutral\", \"relief\", \"sadness\", \"surprise\", \"worry\"], axis = 1)\n",
    "two = pd.concat([two, is_happy, is_sad, is_neutral], axis=1, sort=False)\n",
    "two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
